\documentclass[a4paper,11pt,twoside]{article}
%\documentclass[a4paper,11pt,twoside,se]{article}

\usepackage{UmUStudentReport}
\usepackage{verbatim}   % Multi-line comments using \begin{comment}
\usepackage{courier}    % Nicer fonts are used. (not necessary)
\usepackage{pslatex}    % Also nicer fonts. (not necessary)
\usepackage[pdftex]{graphicx}   % allows including pdf figures
\usepackage{listings}
%\usepackage{lmodern}   % Optional fonts. (not necessary)
%\usepackage{tabularx}
%\usepackage{microtype} % Provides some typographic improvements over default settings
%\usepackage{placeins}  % For aligning images with \FloatBarrier
%\usepackage{booktabs}  % For nice-looking tables
%\usepackage{titlesec}  % More granular control of sections.

% DOCUMENT INFO
% =============
\department{Institution f√∂r Datavetenskap}
\coursename{Datavetenskapens byggstenar 7.5 p}
\coursecode{DV160HT15}
\title{OU4 Analysis of Complexity}
\author{Lorenz Gerber  ({\tt{dv15lgr@cs.umu.se}})}
\date{2015-12-25}
%\revisiondate{2015-09-15}
\instructor{Lena Kallin Westin / Johan Eliasson}


% DOCUMENT SETTINGS
% =================
\bibliographystyle{plain}
%\bibliographystyle{ieee}
\pagestyle{fancy}
\raggedbottom
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
%\graphicspath{{images/}}   %Path for images

\usepackage{float}
\floatstyle{ruled}
\newfloat{listing}{thp}{lop}
\floatname{listing}{Listing}


% DEFINES
% =======
%\newcommand{\mycommand}{<latex code>}

% DOCUMENT
% ========
\begin{document}
\lstset{language=C}
\maketitle

\tableofcontents
\newpage

\section{Introduction} 
The aim with this laboration was to apply experimental and asymptotic
complexity analysis of algorithms. This type of analysis is used to
find out how a algorithm of interest behaves when scaling up the input
size. The analysis will answer the question: In which relation is
the algorithm runtime to the size of input data. A practical 
example is to describe the needed time for sorting a list, dependent 
on the number of list elements. Such an analysis results in a algebraic
expression, a function $f(n) = t$ where $n$ is the input size and $t$
the algorithm runtime. This will allow to classify algorithms into a
number of broad classes according to the most dominant term in the 
function: $k, log(n)$, $n$, $n^2$, $n^k$ or $a^n$.

In this laboration, we were experimenting with two common ways for
determining the function relation between input size and runtim:
experimental complexity analysis and asymptotic complexity analysis
based on calculation of primitive operations. While both of those
methods can yield rather complex expressions, the aim is to find a
simple formula that describes the relation 'on the safe side', i.e. a
conservative estimation of run-time. This last description, finding
a simple, conservative expression for the relation of input size and
runtime qualifies as a `sloppy' definition of the `Big O' notation
which will be described in more detail further down.

\subsection{Experimental Complexity Analysis}
In experimental complexity analysis, the time consumption of the
algorithm is measured for a number of different input sizes. The value
pairs are then visualized in a scatter plot. It is important to choose
apropriate ranges of input size $n$ but also a reasonable number of
replicate measurments for each input size. 
The replication is important for mainly two reasons: First, the performance of a computer
is not constant. It varies within a small range and measurments will
therefore spread somewhat around one theoretical `true' value. Second,
and in most cases more important, many algorithms have a certain
`random' component built in. Alternatively, the test data could be
generated randomly to assess for an average case. Taking up the
example of sorting a list, the data could be already sorted in the
best case or oposite to the requested sort sequence in the worst
case. In any case, replicate measurments for the same input size are
inavoidable.

After plotting the experimental data, assumptions about the relation
between input size and runtime are made. They can be visually tested
by transforming the experimental data with the inverse of the expected
function. If, for example a graph looks like a second degree polynom,
transforming the response variable with the square-root should yield a
straight line, which visually is much easier to identify as such than
a curve. If the assumption was correct and the resulting graph looks
linear, a linear regression analysis will numerically provide proof
for the assumption. The obtained line equation can then be
transformed back, in the described case resulting in a second degree 
polynom. From here, the next step will be to determine `ordo' of the
obtained function which will be described further down.

\subsection{Asymptotic Complexity Analysis}
In asymptotic complexity analysis, the aim is the same as described
above in experimental complexity analysis: Obtaining a algebraic
relation between input data size and runtime. However, in asymptotic
complexity analysis, the algorithm must be know and available. The
analysis is based on a detailed account for how often each step in the
algorithm will run as a function of input size $n$. There can be
certain steps in an algorithm, that will be independent of input
size, constant. Others will be related linear, polynomal, logarithmic
or exponential with input size. A Typical example is a double
\verb!For! loop: It will yield a $n /times n$ relation, hence it
results in a $n^2$ funtion.

The steps to account for in an algorithm are called 'primtive
operations'. This is a rather theoretical definition that leaves some
room for interpretation. But in general, primitive operations are
\verb![read]!, \verb![write/assign]!, \verb![compare]!,
\verb![increase/decrease]!, \verb![arithmetic operations]! etc. Summing up
all primitive operations for a given algorithm will result in a
function $f(n)$ where $n$ is the input size.

\subsection{The `Big O' notation'}
`Ordo' or `Big O' notation is a mathematical definition of an algorithms
time complexity. It's formal defintion is shown in equation
(\ref{eq:ordo})\cite[pp. 245]{janlert2000}.  

\begin{equation} \label{eq:ordo}
f(n) \Rightarrow O(g(n)) \textrm{ if } f(n) \leq c \times g(n)
\textrm{ for } n \geq n_{0} \textrm{ and }
c > 0 \textrm{ and } n_{0} \geq 1
\end{equation}

In words: If the function $c \times g(n)$ for $n$ larger than $n_{0}$
is always bigger than $f(n)$, it is $O(g(n))$ (say: $ordo$  $g$ of
$n$). Hence knowing `ordo' of an algorithm gives a good estimation 
of it's time complexity.

\subsection{Determining of \textit{Ordo}, $c$ and $n_{0}$}
Determing `Ordo' is done in several steps. First the most dominant
function terms of $f(n)$ has to be found and set as $g(n)$. In a
general second degree polynom, $n^2$ is for example the dominant term.

Then the constant factor $c$ is determined by calculating the limes
from n to infinity of dividing $f(n)$ by $g(n)$ according to
equation (\ref{eq:limes}).  

\begin{equation} \label{eq:limes}
\lim_{n \to \infty} {f(n) \over g(n)} + 1
\end{equation}

The value $n_{0}$ is the lower limit of input size $n$ for which
$g(n)$ will be at least equal or larger than $f(n)$. It is found by
solving equation \ref{eq:nzero}. 

\begin{equation} \label{eq:nzero}
c\times g(n) = f(n)
\end{equation}


\section{Material and Methods}

\subsection{Experimental Complexity Analysis}
We obtained a compiled program to run from the command line. The
program would take three arguments: a selector for one of two
implemented algorithms, size of in data and number of repetitions. It
was unknown to us which algorithms were implemented in the
program. The program would run a certain time and before quitting, 
print the runtime to standard out.

To automate the experimental process, a shell script (listing \ref{ls:bash}) 
was written that run the program 10 times for the input sizes n = (1'000, 
2'500, 5'000, 10'000, 15'000, 20'000, 25'000, 50'000, 75'000, 100'000,
125'000, 150'000, 200'000).

\begin{listing}

\begin{verbatim}
#!/usr/local/bin/bash                                                                                           
ARRAY=(1000 2500 5000 10000 15000 20000 25000 50000 75000  100000 125000 150000 200000)
for a in ${ARRAY[*]}
do
    for b in {1..10}
    do
        ./algorithms_osx -2 $a
    done
done
\end{verbatim}
\caption{Shell script to automate the collection of experimental
  data. Standard output was piped and appended to a text file.\label{ls:bash}}
\end{listing}

The script was run from the bash commandline on a MacbookPro8,1 and
the output piped and appended to a text file which was then imported
to statistical computing environment R \cite{rlanguage}. All data analysis and
plotting was done in R. 


\subsection{Asymptotic Complexity Analysis}
We were given pseudo code of a bubble sort algorithm (listing \ref{ls:bubble})
to conduct asymptotic complexity analysis. A short description on how
to account for primitive operations was available through the course
homepage \cite{complex}.

\begin{listing}
\begin{verbatim}
Algorithm bubbleSort(numElements, list[])
input:  numElements, the number of elements in the list
        list, a list of numbers to be sorted
output: the sorted list

1:  done <- false
2:  n <- 0
3:   while (n < numElements) and (done = false)
4:      done <- true
5:      for m <- (numElements -1) downto n
6:          if list[m] < list[m - 1] then
7:              tmp <- list[m]
8:              list[m] <- list[m - 1]
9:              list[m - 1] <- tmp
10:             done <- false
11:    n <- n + 1
12: return list 
\end{verbatim}
\caption{The given pseudo code of a bubble sort.}
\end{listing}

After accounting for primitive operations and obtaining algebraic
functions describing `best' and `worst' case for the bubblesort algorithm, 
`ordo' expressions were determined according to lecture notes. 


\section{Results}
\subsection{Experimental Complexity Analysis}

\begin{figure} 
\includegraphics[width=\textwidth]{graph1.png}
\caption{\textit{Runtime values in seconds for a series of \textit{n}
  repetitions of the investigated algorithm. The left panel shows the
  measured times. For the right panel, the square root for each time
  value was calculated before plotting.}\label{ls:bubble}}
\label{fig:graph}
\end{figure}

The left panel of figure \ref{fig:graph} shows the experimental
runtime values plotted against $n$ length of input data. The values
were then transformed by calculating the square root. The 
transformed data is shown in the right panel of figure 
\ref{fig:graph}. Then linear regression was calculated on the
transformed data (\textit{table\ref{tab:regression}}).

\begin{table}[]
\caption{\textit{Linear regression of the transformed experimental data}}
\label{tab:regression}
\begin{tabular}{lll}

gradient &  & $7.348 \times 10^{-5}$ \\
intercept &  & $1.393 \times 10^{-2}$ \\
$R^2$ &  & 0.9988 \\ 
\end{tabular}
\end{table}

The obtained linear equation was transformed back to yield a
quadratic function as shown in equation (\ref{eq:quadratic}).

\begin{equation} \label{eq:quadratic}
y = 5.4 \times 10^{-9}x^2 + 2.1 \times 10^{-6}x + 0.2 \times 10^{-4}
\end{equation}

Then $c$ was determined according to equation (\ref{eq:limes}) and
$n_0$ was determined by solving the quadratic equation resulting from
$f(n) = g(n)$ (Table \ref{tab:ordo}, figure \ref{fig:ordo}). 

\begin{table}[]
\caption{\textit{Calculated value for ordo determination}}
\label{tab:ordo}
\begin{tabular}{lll}
$c$                       &  & $6.4 \times 10^{-9}$ \\
\multicolumn{1}{c}{$n_0$} &  & 2060                
\end{tabular}
\end{table}


\begin{figure} 
\includegraphics[width=\textwidth]{ordo.png}
\caption{\textit{Comparison of $f(x)$ and $O(g(x))$ in the range of $n_{0}$
  which was determined to be at about $n = 2600$.}}
\label{fig:ordo}
\end{figure}

\subsection{Asymptotic Complexity Analysis}
\subsubsection{Worst Case}
Listing \ref{ls:worst} shows the determination of primitive operation
for the `worst' case. The line numbers correspond to the line numbers
in the original pseudocode shown in listing \ref{ls:bubble}.

\begin{listing}
\begin{verbatim}

1:  1 * [<-] + 
2:  1 * [<-] + 
3:  (numElements + 1) * 
    (3 * [get] + 1 * [<] +  1 * [=] + 1 * [AND]) + 
4:  numElements * (1 * [<-]) +
5:  init:
    (numElements * (numElements -1) / 2 + 1) *
    (1 * [get] + 1 * [-] + 1 * [<-]) +
 
    cond success + counter: 
    numElements * (numElements - 1) / 2) *
    (2 * [get] + 1 * [>] + 1 * [--]) +

    cond fail:
    numElements * 
    (2 * [get n] + 1 * [>]) +

6:  (numElements * (numElements - 1) / 2) * 
    (2 * [get] + 2 * [list[]] + 1 * [-] + 1 * [<] + 
7:      1 * [get] + 1 * [list[]] + 1 * [<-] +
8:      1 * [get] + 1 * [-] + 1 * [list[]] + 1 * [<-] + 
9:      2 * [get] + 1 * [-] + 1 * [ <-] +
10:     1 * [<-] ) +
11: numElements * (1 * [get] + 1 * [+] + 1 * [<-] +
12: 1 * return

set numElements = x

1:  1 + 
2:  1 + 
3:  (x + 1) * 6  
4:  x * 1
5:  (x * (x-1) / 2 + 1) * 3 +
    (x * (x - 1) / 2) * 4 +
    x * 3 +
6:  (x * (x - 1) / 2) * (6 +
7:    3 + 
8:    4 +
9:    4 + 
10:   1) + 
11: x * 3 +
12: 1 

Hence:
1 + 1 + 6x + 6 + x + 1.5x^2 - 1.5x + 3 + 2x^2 - 2x + 
3x + 9x^2 - 9x + 3x + 1

= 12.5x^2 + 4.5x + 12

\end{verbatim}
\caption{\textit{Determining the `worst case' complexity for the given
    `bubblesort' algorithm. The line numbers correspond to those in 
listing \ref{ls:bubble}}.\label{ls:worst}}
\end{listing}

\subsubsection{Best case}
Listing \ref{ls:best} shows the determination of primitive operation
for the `best' case. The line numbers correspond to the line numbers
in the original pseudocode shown in listing \ref{ls:bubble}.

\begin{listing}
\begin{verbatim}
1:  1 * [<-] +
2:  1 * [<-] +
3:  2 * (3 * [get] + 1 *[<] + 1 * [and] + 1* [==]) +
4:  1 * [<-] +
5:  init:
    1 * [get] + 1 * [-] + 1 * [<-] +

    cond success + counter:
    (numElements - 1) * (1 * [get] + 1 * [>]) + 1 * [--]) +
    
    cond fail:
    1* [get] + 1 * [>] + 
6:  (numElements - 1) * 
    (2 * [get] +  2 * list[] + 1 * [-] + 1 * [<]) +
11: 1 * [get] + 1 * [+] + 1 * [<-]
12: 1 * [return]

set numElements = x

1:  1 +
2:  1 +
3:  2 * 6 +
4:  1 +
5:  3 + 
    (x - 1) * 3 +
    2 +
6:  (x - 1) * 6 +
11: 3 +
12: 1 

Hence:
1 + 1 + 12 + 1 + 3 + 3x - 3 + 2 + 6x -6 + 3 + 1
= 9x + 15

\end{verbatim}
\caption{Determining the `best case' complexity for the given
  `bubblesort' algorithm. The line numbers correspond to those in
  listing \ref{ls:bubble}.\label{ls:best}}
\end{listing}

In table \ref{tab:ordo2} the keyfigures of the `ordo' determination for
`best' and `worst' case expressions can be found. 

\begin{table}[]
\caption{\textit{Determination of $ordo$ for `Best' and `Worst Case'}}
\label{tab:ordo2}
\begin{tabular}{llcc}
&  & Worst Case & Best Case \\ 
\hline
$f(n)$ &  & \multicolumn{1}{c}{$12.5n^2+4.5x+12$} & \multicolumn{1}{c}{$9x+15$} \\
$g(n)$ &  & \multicolumn{1}{c}{$13.5n^2$}         & \multicolumn{1}{c}{$10x$}   \\
$c$    &  & 13.5                                  & 10                          \\
$n_0$  &  & 7                                     & 15                          \\                         
\end{tabular}
\end{table}

\section{Discussion}
\subsection{Experimental Complexity Analysis}
Plotting the experimental data (left panel of figure \ref{fig:graph}) showed that
the investigated algorithm is not of $O(k)$ or $O(n)$ as this would
have yielded a straight line. To test the hypothesis whether the
algorithm could follow a second degree polynom function, the response
variabel was transformed. If the hypothesis is true, one can expect
the resulting plot to expose linear behaviour. As can be seen in the
right panel of figure \ref{fig:graph}, this was the case. At least
visually, the obtained data looked linear. To get a more objective
metric, linear regression was calculated on the transormed data. The
obtained $R^2$ value of 0.9988 is a very strong indication that the
transformed data is well represented by a linear equation 
(table \ref{tab:regression}). So the hypothesis that the obtained
experimental data follows a second degree polynom function was
seen as proven and the linear regression equation was transformed back
to yield a second degree polynom function (equation
\ref{eq:quadratic}). Finally, $c$ and $n_{0}$ were calculated to confirm
the validity of the found `ordo' defintion (table \ref{tab:ordo}). 


\subsection{Asymptotic Complexity Analysis}
The main given pseudo code for a bubble sort algorithm has the ma

\subsubsection{Worst Case}



Line 1, 2 and 12 run just once. Line 3 runs \verb!numElements + 1! times. Lines
4 and 11 run \verb!numElements! times. Line 5 runs
\verb!(numElements * (numElements - 1)) / 2) + 1! 
times. The lines 6 to 10 run \verb!numElements * (numElements - 1) / 2! times. 
The \verb!downto! in \verb!for m <- (numElements - 1) downto n! was
interpreted as `larger than' condition.


\addcontentsline{toc}{section}{\refname}
\bibliography{references}

\end{document}
